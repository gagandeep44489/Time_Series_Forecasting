{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6768,"databundleVersionId":44342,"sourceType":"competition"}],"dockerImageVersionId":29860,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Det der først og fremmest gør denne opgave svær, er faktisk at den er svær at forstå. Hvad er det der skal afleveres, hvordan stemmer submission-id'erne overens med predictions? Hvorfor ligger datoerne på den anden led etc.","metadata":{}},{"cell_type":"markdown","source":"Hvis vi undersøger, hvor lange træningsfilerne er, viser det sig at de er 145063 rows, og når vi ser, hvad der skal predictes er det 62 dage (fra 13. september til og med 13. november), det giver 8993906 entries, hvilket stemmer overens med længden i key og submission-filerne. Det betyder, at hver page skal angive sin prediction for hver af de 62 dage. Desværre er det ikke sådan, at de første 62 entries svarer til de 62 dage for den første page i sættet, så vi bliver nødt til at finde koblingen mellem page og submission_id før vi kan aflevere noget.\nDe 145063 er individuelle time series for hver page der er blevet besøgt","metadata":{}},{"cell_type":"markdown","source":"En god strategi kunne være at starte med kun at arbejde på en row af gangen, da hver row er en time series for en specifik artikel på wikipedia. Desuden kunne en god baseline være medianen per time series, så vi kunne lave et loop, som tager en timeseries af gangen, finder medianen, og sætter værdien ind","metadata":{}},{"cell_type":"markdown","source":"pseudokoden ville være lidt a la:\n>  foreach page in pages \n\n>     for (i=0; i<=61; i++)","metadata":{}},{"cell_type":"markdown","source":"Til at predicte med, kunne vi prøve: ARIMA, Prophet, RandomForestRegressor eller XGBoost (eller noget helt syvende, men vi kender ARIMA og RandomForest, og vi har snakket lidt om XGBoost. Prophet er noget lignende ARIMA, som facebook har udviklet). Husk at redegøre for, hvordan den algoritme I anvender virker.","metadata":{}},{"cell_type":"code","source":"from fbprophet import Prophet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/web-traffic-time-series-forecasting/train_1.csv.zip\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_pages = train['Page']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first_page = all_pages[0]\nfirst_page","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Transpose for at vende matricen, reset_index for at starte fra nul, så datoerne ikke bliver index, og omdøb index til Date, så vi kan se at de er datoer","metadata":{}},{"cell_type":"code","source":"train_allT = train.set_index('Page').T.reset_index().rename(columns={'index':'Date'})\ntrain_allT.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Kolonnerne skal hedde ds og y hvis vi vil bruge prophet","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(train_allT, columns = ['Date',first_page]) \ndf = df.rename(columns={'Date':'ds', first_page:'y'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m = Prophet()\nm.fit(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"future = m.make_future_dataframe(periods=60)\nfuture.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forecast = m.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Her er resultatet for den ene time series, det samme skal så gøres med alle de andre, og så skal vi have dem placeret de rigtige steder i submission-filen","metadata":{}},{"cell_type":"code","source":"result = forecast['yhat']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Resultatat viser hele datasættet inklusive de data vi har trænet ud fra, derfor er sættet 610 langt. Men vi skal kun bruge de sidste 60 entries","metadata":{}},{"cell_type":"code","source":"sub_result = result[-60:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_result.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig1 = m.plot(forecast)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ud fra plottet kan vi se, at der er nogle outliers, men vi kunne jo godt prøve bare at levere denne simple løsning for at se, hvad det giver","metadata":{}},{"cell_type":"code","source":"fig2 = m.plot_components(forecast)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"key = pd.read_csv(\"/kaggle/input/web-traffic-time-series-forecasting/key_1.csv.zip\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"key","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Find liste af id'er vi skal bruge","metadata":{}},{"cell_type":"code","source":"listOfIds = key.index[key['Page'].str.contains(first_page)].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"listOfIds.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv(\"/kaggle/input/web-traffic-time-series-forecasting/sample_submission_1.csv.zip\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['Visits'].loc[listOfIds.min():listOfIds.max()] = sub_result.values.round()\n#sub['Visits'].loc[0:59] = sub_result.values.round()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def my_function():\n    for page in all_pages:\n        #print(page)\n        df = pd.DataFrame(train_allT, columns = ['Date',page]) \n        df = df.rename(columns={'Date':'ds', page:'y'})\n        m = Prophet()\n        m.fit(df)\n        future = m.make_future_dataframe(periods=60)\n        forecast = m.predict(future)\n        result = forecast['yhat']\n        sub_result = result[-60:]\n        listOfIds = key.index[key['Page'].str.contains(page)].values\n        sub['Visits'].loc[listOfIds.min():listOfIds.max()] = sub_result.values.round()\n        #print(sub)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#my_function()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sub.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Læs ind","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nprint('Reading data...')\nkey_1 = pd.read_csv('/kaggle/input/web-traffic-time-series-forecasting/key_2.csv.zip')\ntrain_1 = pd.read_csv('/kaggle/input/web-traffic-time-series-forecasting/train_2.csv.zip')\nss_1 = pd.read_csv('/kaggle/input/web-traffic-time-series-forecasting/sample_submission_2.csv.zip')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lav lister af id'er og page-navne med dato på","metadata":{}},{"cell_type":"code","source":"print('Preprocessing...')\n# train_1.fillna(0, inplace=True)\n\nprint('Processing...')\nids = key_1.Id.values\npages = key_1.Page.values\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"fjern dato-delen fra alle page-navne","metadata":{}},{"cell_type":"code","source":"print('key_1...')\nd_pages = {}\nfor id, page in zip(ids, pages):\n    d_pages[id] = page[:-11]\n   \n    \n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('train_1...')\npages = train_1.Page.values\n# visits = train_1['2016-12-31'].values # Version 1 score: 60.6\n# visits = np.round(np.mean(train_1.drop('Page', axis=1).values, axis=1)) # Version 2 score: 64.8\n# visits = np.round(np.mean(train_1.drop('Page', axis=1).values[:, -14:], axis=1)) # Version 3 score: 52.5\n# visits = np.round(np.mean(train_1.drop('Page', axis=1).values[:, -7:], axis=1)) # Version 4 score: 53.7\n# visits = np.round(np.mean(train_1.drop('Page', axis=1).values[:, -21:], axis=1)) # Version 5, 6 score: 51.3\n# visits = np.round(np.mean(train_1.drop('Page', axis=1).values[:, -28:], axis=1)) # Version 7 score: 51.1\n# visits = np.round(np.median(train_1.drop('Page', axis=1).values[:, -28:], axis=1)) # Version 8 score: 47.1 \n# visits = np.round(np.median(train_1.drop('Page', axis=1).values[:, -35:], axis=1)) # Version 9 score: 46.6\n# visits = np.round(np.median(train_1.drop('Page', axis=1).values[:, -42:], axis=1)) # Version 10 score: 46.3\n# visits = np.round(np.median(train_1.drop('Page', axis=1).values[:, -49:], axis=1)) # Version 11 score: 46.2\n# visits = np.nan_to_num(np.round(np.nanmedian(train_1.drop('Page', axis=1).values[:, -49:], axis=1))) # Version 12 score: 45.7\nvisits = np.nan_to_num(np.round(np.nanmedian(train_1.drop('Page', axis=1).values[:, -56:], axis=1))) # scorer 41.8 #find medianen de sidste 56 dage og skift nan ud med 0\n\nd_visits = {}\nfor page, visits_number in zip(pages, visits):\n    d_visits[page] = visits_number\n    # for hver page i pages og visit i visits gem antal visits på page\n\nprint('Modifying sample submission...') # læs submissionfilen ind\nss_ids = ss_1.Id.values\nss_visits = ss_1.Visits.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d_visits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d_pages","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss_ids","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nfor i, ss_id in enumerate(ss_ids):\n    ss_visits[i] = d_visits[d_pages[ss_id]] #sæt første ss_id i d_pages-listen for at finde page-navn. sæt så page-navn i d_visits for at finde tal-værdien, \n    #som gemmes i stedet for ss_tal-værdien inkrementalt.\n\nprint('Saving submission...')\nsubm = pd.DataFrame({'Id': ss_ids, 'Visits': ss_visits})\nsubm.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Id'erne er en kombination af dato og page","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Pre-processing and feature engineering train data...')\ntrain_flattened = pd.melt(train[list(train.columns[-49:])+['Page']], id_vars='Page', var_name='date', value_name='Visits')\ntrain_flattened['date'] = train_flattened['date'].astype('datetime64[ns]')\ntrain_flattened['weekend'] = ((train_flattened.date.dt.dayofweek) // 5 == 1).astype(float)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(50, 8))\nmean_group = train_flattened[['Page','date','Visits']].groupby(['date'])['Visits'].mean()\nplt.plot(mean_group)\nplt.title('Time Series - Average')\nplt.show()\nplt.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"times_series_means =  pd.DataFrame(mean_group).reset_index(drop=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_date_index = times_series_means[['date','Visits']].set_index('date')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\n# Run Dicky-Fuller test\nresult = adfuller(df_date_index)\n\n# Print test statistic\nprint(result[0])\n\n# Print p-value\nprint(result[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n# Create figure\nfig, (ax1, ax2) = plt.subplots(2,1, figsize=(12,8))\n \n# Plot the ACF of savings on ax1\nplot_acf(df_date_index, zero=False, ax=ax1, lags=10)\n\n# Plot the PACF of savings on ax2\nplot_pacf(df_date_index, zero=False, ax=ax2, lags=10)\n\nplt.show()\nplt.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create empty list to store search results\norder_aic_bic=[]\n\n# Loop over p values from 0-2\nfor p in range(3):\n  # Loop over q values from 0-2\n    for q in range(3):\n        try:\n            # create and fit ARMA(p,q) model\n            model = SARIMAX(df_date_index, order=(p,0,q), seasonal_order=(1,2,0,7))\n            results = model.fit()\n           \n\n            # Append order and results tuple\n            order_aic_bic.append((p,q, results.aic, results.bic))\n            print(p,q,results.aic, results.bic)\n            \n        except:\n            print(p, q, None, None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n# Create and fit model\nmodel = SARIMAX(df_date_index, order=(2,0,1), trend='c')\nresults = model.fit()\n\n# Create the 4 diagostics plots\nresults.plot_diagnostics()\nplt.show()\nplt.close()\n\n# Print summary\nprint(results.summary())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import seasonal decompose\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# Perform additive decomposition\ndecomp = seasonal_decompose(df_date_index, \n                            freq=7)\n\n# Plot decomposition\ndecomp.plot()\nplt.show()\nplt.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pmdarima","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pmdarima as pm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create auto_arima model\nmodel1 = pm.auto_arima(df_date_index,\n                      seasonal=True, m=7,\n                      d=0, D=1, \n                 \t  max_p=2, max_q=2,\n                      trace=True,\n                      error_action='ignore',\n                      suppress_warnings=True)\n                       \n# Print model summary\nprint(model1.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import model class\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n# Create model object\nmodel = SARIMAX(df_date_index, \n                order=(2,0,1), \n                seasonal_order=(1,1,1,7), \n                trend='c')\n# Fit model\nresults = model.fit()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot common diagnostics\nresults.plot_diagnostics()\nplt.show()\nplt.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create forecast object\nforecast_object = results.get_forecast(steps=90)\n\n# Extract prediction mean\nmean = forecast_object.predicted_mean\n\n# Extract the confidence intervals\nconf_int = forecast_object.conf_int()\n\n# Extract the forecast dates\ndates = mean.index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf_date_index.index = pd.to_datetime(df_date_index.index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print last predicted mean\nprint(mean.iloc[-1])\n\n# Print last confidence interval\nprint(conf_int.iloc[-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Validating Forecast\npred = results.get_prediction(start=pd.to_datetime('2016-12-01'), dynamic=False)\npred_ci = pred.conf_int()\nax = df_date_index['2016':].plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.2)\nax.set_xlabel('Date')\nax.set_ylabel('Sales')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_forecasted = pred.predicted_mean\ny_truth = df_date_index['2016-10-01':]\nmse = ((y_forecasted - y_truth) ** 2).mean()\nprint('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))\n#The MSE is a measure of the quality of an estimator — it is always non-negative, \n#and the smaller the MSE, the closer we are to finding the line of best fit.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_uc = results.get_forecast(steps=100)\npred_ci = pred_uc.conf_int()\nax = df_date_index.plot(label='observed', figsize=(14, 7))\npred_uc.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_xlabel('Date')\nax.set_ylabel('Sales')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}